# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RUi9FYV_VciRnUPKTmK1bhFPqbYPP5t2
"""

!pip install gymnasium stable-baselines3 requests

import requests
import numpy as np

BASE_URL = "https://e4bfd2ae92b1.ngrok-free.app"  # your ngrok URL

def get_see_from_server(x, y, z):
    """Send (x, y, z) to your Flask+MATLAB server and get SEE."""
    url = BASE_URL + "/see"
    payload = {"x": float(x), "y": float(y), "z": float(z)}
    try:
        resp = requests.post(url, json=payload, timeout=60)
        resp.raise_for_status()
        data = resp.json()
        return float(data["see"])
    except Exception as e:
        print("ERROR contacting server:", e)
        return 0.0

import requests

BASE_URL = "https://e4bfd2ae92b1.ngrok-free.app"  # your URL

r = requests.get(BASE_URL + "/ping")
print(r.status_code, r.text)

import requests
import numpy as np

BASE_URL = "https://e4bfd2ae92b1.ngrok-free.app"  # put your current ngrok URL here

def get_see_from_server(x, y, z):
    url = BASE_URL + "/see"
    payload = {"x": float(x), "y": float(y), "z": float(z)}
    try:
        resp = requests.post(url, json=payload, timeout=60)
        resp.raise_for_status()
        data = resp.json()
        return float(data["see"])
    except Exception as e:
        print("ERROR contacting server:", e)
        try:
            print("Server response:", resp.text)
        except:
            pass
        return 0.0

print(get_see_from_server(10, 20, 30))

!pip install gymnasium stable-baselines3

import gymnasium as gym
from gymnasium import spaces

class UavSeeEnv(gym.Env):
    """
    3D UAV environment:
    - Observation: [x, y, z] ∈ [-1,1]^3  (normalized)
    - Action: Δx, Δy, Δz ∈ [-1,1]^3 (scaled by step_size)
    - Reward: SEE from MATLAB via Flask/ngrok
    """
    metadata = {"render_modes": ["human"]}

    def __init__(self,
                 x_range=(0.0, 100.0),
                 y_range=(0.0, 100.0),
                 z_range=(10.0, 40.0),
                 max_steps=30,
                 step_size=5.0):
        super().__init__()

        self.x_min, self.x_max = x_range
        self.y_min, self.y_max = y_range
        self.z_min, self.z_max = z_range

        self.max_steps = max_steps
        self.step_size = step_size

        # Action: Δx, Δy, Δz ∈ [-1, 1]
        self.action_space = spaces.Box(
            low=np.array([-1.0, -1.0, -1.0], dtype=np.float32),
            high=np.array([1.0,  1.0,  1.0], dtype=np.float32),
            dtype=np.float32
        )

        # Observation: normalized [x, y, z]
        self.observation_space = spaces.Box(
            low=-1.0,
            high=1.0,
            shape=(3,),
            dtype=np.float32
        )

        self.pos = None
        self.steps = 0

    def _normalize_pos(self, x, y, z):
        nx = 2*(x - self.x_min)/(self.x_max - self.x_min) - 1
        ny = 2*(y - self.y_min)/(self.y_max - self.y_min) - 1
        nz = 2*(z - self.z_min)/(self.z_max - self.z_min) - 1
        return np.array([nx, ny, nz], dtype=np.float32)

    def _clip_pos(self, x, y, z):
        x = np.clip(x, self.x_min, self.x_max)
        y = np.clip(y, self.y_min, self.y_max)
        z = np.clip(z, self.z_min, self.z_max)
        return x, y, z

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        self.steps = 0
        # start at center of box
        x0 = 0.5*(self.x_min + self.x_max)
        y0 = 0.5*(self.y_min + self.y_max)
        z0 = 0.5*(self.z_min + self.z_max)
        self.pos = np.array([x0, y0, z0], dtype=np.float32)
        obs = self._normalize_pos(*self.pos)
        info = {}
        return obs, info

    def step(self, action):
        self.steps += 1

        dx, dy, dz = action * self.step_size
        x, y, z = self.pos
        x += float(dx)
        y += float(dy)
        z += float(dz)

        x, y, z = self._clip_pos(x, y, z)
        self.pos = np.array([x, y, z], dtype=np.float32)

        see = get_see_from_server(x, y, z)
        reward = see

        obs = self._normalize_pos(x, y, z)

        terminated = False
        truncated = self.steps >= self.max_steps
        info = {"x": x, "y": y, "z": z, "see": see}

        return obs, reward, terminated, truncated, info

env = UavSeeEnv()
obs, info = env.reset()
print("Initial obs:", obs)

a = env.action_space.sample()
obs, r, terminated, truncated, info = env.step(a)
print("Reward (SEE):", r)
print("Info:", info)

from stable_baselines3 import PPO
from stable_baselines3.common.env_util import DummyVecEnv

vec_env = DummyVecEnv([lambda: UavSeeEnv()])

model = PPO(
    "MlpPolicy",
    vec_env,
    verbose=1,
    learning_rate=3e-4,
    gamma=0.99,
    n_steps=128,
    batch_size=64,
)

model.learn(total_timesteps=5000)  # later you can increase this
model.save("ppo_uav_see_agent")

import pandas as pd

env = UavSeeEnv()
obs, info = env.reset()
traj = []

for t in range(30):  # one episode
    action, _ = model.predict(obs, deterministic=True)
    obs, reward, terminated, truncated, info = env.step(action)
    traj.append({
        "t": t,
        "x": info["x"],
        "y": info["y"],
        "z": info["z"],
        "see": info["see"],
    })
    if terminated or truncated:
        break

df_traj = pd.DataFrame(traj)
df_traj

import pandas as pd

env = UavSeeEnv()
obs, info = env.reset()
traj = []

for t in range(30):  # one episode
    action, _ = model.predict(obs, deterministic=True)
    obs, reward, terminated, truncated, info = env.step(action)
    traj.append({
        "t": t,
        "x": info["x"],
        "y": info["y"],
        "z": info["z"],
        "see": info["see"],
    })
    if terminated or truncated:
        break

df_traj = pd.DataFrame(traj)
df_traj

print("Average SEE along DRL path:", df_traj["see"].mean())
print("Max SEE along DRL path:", df_traj["see"].max())
print("Position of max SEE:")
print(df_traj.loc[df_traj["see"].idxmax()])

def rollout_random(env, horizon=30):
    obs, info = env.reset()
    traj = []
    for t in range(horizon):
        action = env.action_space.sample()  # random step
        obs, reward, terminated, truncated, info = env.step(action)
        traj.append({
            "t": t,
            "x": info["x"],
            "y": info["y"],
            "z": info["z"],
            "see": info["see"],
        })
        if terminated or truncated:
            break
    return pd.DataFrame(traj)

rand_env = UavSeeEnv()
df_rand = rollout_random(rand_env)
print("Random-walk avg SEE:", df_rand["see"].mean())
df_rand.head()

USER_TARGET = np.array([47.0, 47.0, 10.0], dtype=np.float32)

def greedy_towards_user_action(pos, step_size):
    # vector from UAV to user target
    direction = USER_TARGET - pos
    norm = np.linalg.norm(direction)
    if norm == 0:
        return np.zeros(3, dtype=np.float32)
    unit = direction / norm
    # action is unit vector scaled so that step ≈ step_size
    # environment multiplies by step_size, so we scale back
    return np.clip(unit, -1.0, 1.0).astype(np.float32)

def rollout_greedy(env, horizon=30):
    obs, info = env.reset()
    traj = []
    for t in range(horizon):
        pos = env.pos  # current (x,y,z)
        action = greedy_towards_user_action(pos, env.step_size)
        obs, reward, terminated, truncated, info = env.step(action)
        traj.append({
            "t": t,
            "x": info["x"],
            "y": info["y"],
            "z": info["z"],
            "see": info["see"],
        })
        if terminated or truncated:
            break
    return pd.DataFrame(traj)

greedy_env = UavSeeEnv()
df_greedy = rollout_greedy(greedy_env)
print("Greedy avg SEE:", df_greedy["see"].mean())
df_greedy.head()

print("DRL avg SEE:", df_traj["see"].mean())
print("Random avg SEE:", df_rand["see"].mean())
print("Greedy avg SEE:", df_greedy["see"].mean())

import matplotlib.pyplot as plt

plt.figure()
plt.plot(df_traj["t"], df_traj["see"], label="DRL")
plt.plot(df_rand["t"], df_rand["see"], label="Random")
plt.plot(df_greedy["t"], df_greedy["see"], label="Greedy")
plt.xlabel("Time step")
plt.ylabel("SEE")
plt.legend()
plt.title("SEE vs Time for Different Policies")
plt.show()

plt.figure()
plt.plot(df_traj["x"], df_traj["y"], marker="o", label="DRL")
plt.plot(df_rand["x"], df_rand["y"], marker="x", label="Random")
plt.plot(df_greedy["x"], df_greedy["y"], marker="s", label="Greedy")
plt.xlabel("X (m)")
plt.ylabel("Y (m)")
plt.legend()
plt.title("UAV Trajectories in X–Y Plane")
plt.gca().set_aspect("equal", adjustable="box")
plt.show()